{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "from algorithms import *\n",
    "from benchmark import *\n",
    "\n",
    "# Definition of variables\n",
    "benchmarkfunction_list = [StybliskiTang, Rastrigin, Rosenbrock, Beale, Sphere, Perm, GoldsteinPrice, Ackley, Bohachevsky] #Hartmann\n",
    "algorithms_list        = [bayesian_optimization, bfgs, latin_hypercube, nelder_mead, particle_swarm, response_surface, sobol]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The benchmark functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the X1 and X2 span\n",
    "X1 = np.linspace(-5, 5, 101)\n",
    "X2 = np.linspace(-5, 5, 101)\n",
    "points = np.array([[x1,x2] for x1 in X1 for x2 in X2]).T\n",
    "\n",
    "fig, ax = plt.subplots(len(benchmarkfunction_list), 2, figsize=(16,40))\n",
    "\n",
    "for idx, fun in enumerate(benchmarkfunction_list):\n",
    "    # Compute the function\n",
    "    Fx = fun(points).reshape(101,101)\n",
    "\n",
    "    ax[idx,0].contourf(X1,X2,Fx)\n",
    "    ax[idx,0].axis('scaled')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithms on test_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the X1 and X2 span\n",
    "# X1 = np.linspace(-5, 5, 101)\n",
    "# X2 = np.linspace(-5, 5, 101)\n",
    "# points = np.array([[x1,x2] for x1 in X1 for x2 in X2])\n",
    "\n",
    "# # Define the parameters for the algorithms\n",
    "# budget    = 100\n",
    "# n_samples = 15\n",
    "# dimension = 2\n",
    "\n",
    "# # Define the intial set of points\n",
    "# initial_x =  np.random.uniform(low=-5, high=5, size=[dimension, n_samples])\n",
    "\n",
    "\n",
    "\n",
    "# fig, ax = plt.subplots(len(algorithms_list), 2*len(benchmarkfunction_list), figsize=(100,50))\n",
    "\n",
    "# plot_idx = 0\n",
    "\n",
    "# for test_function in benchmarkfunction_list:\n",
    "\n",
    "#     # Compute the function\n",
    "#     Fx = test_function(points.T)\n",
    "\n",
    "#     # Find minimum and its coordinates\n",
    "#     idx  = np.argmin(Fx)\n",
    "#     Xmin = points[idx]\n",
    "#     print(f\"[MIN function] Minimum in x={Xmin[0]},    y={Xmin[1]}    with f={Fx[idx]}\")\n",
    "\n",
    "#     Fx = Fx.reshape(101,101)\n",
    "\n",
    "#     for idx, fun in enumerate(algorithms_list):\n",
    "\n",
    "#         if   (idx == 0): # Bayesian Optmization\n",
    "#             X_best, Y_best, X, Y = fun(test_function, X=initial_x, dimension=dimension, n_samples=n_samples, sampling_budget=budget, low=-5, high=5)\n",
    "#             #print(f\"X_best: {X_best},    Y_best: {Y_best}\")\n",
    "            \n",
    "#         elif (idx == 1): # BFGS\n",
    "#             X_best, Y_best, X, Y = fun(test_function, x0=initial_x[:,0], x1=initial_x[:,1], sampling_budget=budget, tol=10e-6)\n",
    "#             #print(f\"X_best: {X_best},    Y_best: {Y_best}\")\n",
    "#             pass\n",
    "#         elif (idx == 2): # Latin Hypercube\n",
    "#             X_best, Y_best, X, Y = fun(test_function, n_samples=budget , dimension=dimension, lower_bounds= [-5, -5], upper_bounds=[5, 5])\n",
    "#             #print(f\"X_best: {X_best},    Y_best: {Y_best}\")\n",
    "\n",
    "#         elif (idx == 3): # Nelder Mead\n",
    "#             X_best, Y_best, X, Y = fun(test_function, x0 = initial_x[:,0], low=-5, high=5, step=0.1, no_improve_thr=10e-6, no_improv_break=10, sampling_budget=budget, alpha=1., gamma=2., rho=-0.5, sigma=0.5)\n",
    "#             #print(f\"X_best: {X_best},    Y_best: {Y_best}\")\n",
    "\n",
    "#         elif (idx == 4): # Particle Swarm\n",
    "#             X_best, Y_best, X, Y = fun(test_function, X=initial_x, V=None, dimension=dimension, low=-5, high=5, sampling_budget=budget, n_particles=n_samples, tol=10e-6, c1=0.1, c2=0.1, w=0.8)\n",
    "#             #print(f\"X_best: {X_best},    Y_best: {Y_best}\")\n",
    "\n",
    "#         elif (idx == 5): # Response Surface Modeling\n",
    "#             X_best, Y_best, X, Y = fun(test_function, x0=initial_x[:,0], sampling_budget=budget, tol = 1e-8, sampling_method=\"box_behnken\", sampling_bound=0.5, iteration_method=\"gradient\", learning_rate=0.01)\n",
    "#             #print(f\"X_best: {X_best},    Y_best: {Y_best}\")\n",
    "#             pass\n",
    "#         elif (idx == 6): # Sobol\n",
    "#             X_best, Y_best, X, Y = fun(test_function, n_samples=budget , dimension=dimension, lower_bounds= [-5, -5], upper_bounds=[5, 5])\n",
    "#             #print(f\"X_best: {X_best},    Y_best: {Y_best}\")\n",
    "\n",
    "\n",
    "#         ax[idx, plot_idx].contourf(X1,X2,Fx)\n",
    "#         ax[idx, plot_idx].axis('scaled')\n",
    "#         ax[idx, plot_idx].scatter(X[:,0], X[:,1], c=\"red\",  s=1)        # The points\n",
    "#         ax[idx, plot_idx].scatter(X[0,0], X[0,1], c=\"blue\", s=3)        # Inital point in the log\n",
    "#         ax[idx, plot_idx].scatter(Xmin[0], Xmin[1], c=\"white\",s=3)      # Real minimum\n",
    "#         ax[idx, plot_idx].scatter(X_best[0], X_best[1],c=\"orange\",s=3)  # Estimated minimum\n",
    "\n",
    "#         ax[idx, plot_idx + 1].plot(np.arange(0,len(Y)), Y)\n",
    "#         ax[idx, plot_idx + 1].set_title(\"Performances over time\")\n",
    "\n",
    "#     plot_idx += 2\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Optimization performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the X1 and X2 span\n",
    "X1 = X2 = np.linspace(-5, 5, 101)\n",
    "points = np.array([[x1,x2] for x1 in X1 for x2 in X2])\n",
    "\n",
    "# Define the parameters for the algorithms\n",
    "budget    = 100\n",
    "n_samples = 15\n",
    "dimension = 2\n",
    "new_max = 0.28\n",
    "new_min = 0.22\n",
    "\n",
    "test_rounds = 25\n",
    "batch_size = 10\n",
    "fig, ax = plt.subplots(2,  len(benchmarkfunction_list), figsize=(30,10))\n",
    "\n",
    "data = [[\"Index\",\"Function\", \"Tests\", \"Error Mean\", \"Error std\"]]\n",
    "\n",
    "for plt_idx, test_function in enumerate(benchmarkfunction_list):\n",
    "    # Compute the function\n",
    "    Fx = test_function(points.T)\n",
    "\n",
    "    # Find minimum and its coordinates\n",
    "    idx  = np.argmin(Fx)\n",
    "    Xmin = points[idx]\n",
    "    Fmin = Fx[idx]\n",
    "    Fmax = np.max(Fx)\n",
    "    test_function_scaled = lambda x: (new_max - new_min)*(test_function(x) - Fmin)/(Fmax - Fmin) + new_min\n",
    "    Fx_scaled = test_function_scaled(points.T)\n",
    "    Fmin = Fx_scaled[idx]\n",
    "    ax[1, plt_idx].contourf(X1,X2,Fx_scaled.reshape(101,101))\n",
    "    ax[1, plt_idx].axis('scaled')\n",
    "    ax[1, plt_idx].scatter(Xmin[0], Xmin[1], c=\"white\",s=3)      # Real minimum\n",
    "\n",
    "    \n",
    "    test_mean, test_std = [], []\n",
    "    for _ in range(test_rounds):\n",
    "        batch = []\n",
    "        for _ in range(batch_size):\n",
    "            # Define the intial set of points\n",
    "            initial_x =  np.random.uniform(low=-5, high=5, size=[dimension, n_samples])\n",
    "            \n",
    "            # Estimate the minimum\n",
    "            X_best, Y_best, _, _ = bayesian_optimization(test_function_scaled, X=initial_x, dimension=dimension, n_samples=n_samples, sampling_budget=budget, low=-5, high=5)\n",
    "            batch.append(Y_best)\n",
    "            ax[1, plt_idx].scatter(X_best[0], X_best[1], c=\"orange\",s=2) \n",
    "        \n",
    "        batch_mean = np.mean(Fmin - batch)\n",
    "        batch_std  = np.std(Fmin - batch)\n",
    "        print(batch_mean)\n",
    "        test_mean.append(batch_mean)\n",
    "        test_std.append(batch_std)\n",
    "    test_mean = np.array(test_mean)\n",
    "    test_std = np.array(test_std)\n",
    "    data.append([test_function.__name__, test_rounds*batch_size, np.mean(test_mean).copy(), np.sqrt(np.sum(test_std**2) / (batch_size-1))])\n",
    "\n",
    "    \n",
    "    ax[0, plt_idx].plot(np.arange(0,len(test_mean)), test_mean, label='Computed minima')\n",
    "    ax[0, plt_idx].fill_between(np.arange(0,len(test_mean)), test_mean - test_std, test_mean + test_std, alpha=0.2, label='Standard Deviation')\n",
    "\n",
    "for i, row in enumerate(data):\n",
    "    if (i==0): print(f\"|{row[0]:<5}|{row[1]:<20}|{row[2]:<10}|{row[3]:<25}|{row[4]:<25}|\")\n",
    "    else:\n",
    "        if (i == 1): print(f\"{'-' * 85}\")\n",
    "        print(f\"|{i:<5}|{row[0]:<20}|{row[1]:<10}|{row[2]:<25}|{row[3]:<25}|\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
